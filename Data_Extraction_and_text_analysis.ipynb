{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(r'E:/sel/chrome_test_profile/Input.xlsx')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(data=df,columns=['URL'])\n",
    "url1 = df['URL'].tolist()\n",
    "#print(url1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data=df,columns=['URL_ID'])\n",
    "url_id1 = df['URL_ID'].tolist()\n",
    "#print(url_id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_essay(title, content, file_name):\n",
    "    \n",
    "    with open(file_name, 'w',encoding=\"utf-8\") as file:\n",
    "        file.write(f\"{title}\\n\")\n",
    "        file.write(content)\n",
    "\n",
    "    #print(f\"--------> Essay [{file_name}] Written\")\n",
    "\n",
    "\n",
    "for i in range(len(url1)):\n",
    "    url = url1[i]\n",
    "    header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'}\n",
    "\n",
    "    response = requests.get(url, headers = header)\n",
    "    html = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #article_title=soup.title.text\n",
    "    \n",
    "    article_title = soup.find('h1', class_= \"entry-title\").text.strip()\n",
    "    main_content = soup.find('div', class_ = \"td-post-content\")\n",
    "\n",
    "    essay = \"\"\n",
    "    for x in main_content.find_all(\"p\"):\n",
    "        article_text=x.get_text()\n",
    "        essay += article_text+\"\\n\"\n",
    "\n",
    "    #print(article_title)\n",
    "    #print(essay)\n",
    "\n",
    "    url_id = url_id1[i]\n",
    "\n",
    "    write_essay(title=article_title, content=essay, file_name=f\"{url_id}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw1=open(\"E:\\sel\\chrome_test_profile\\StopWords_Auditor.txt\", \"r\")\n",
    "sw2 = open(\"E:\\sel\\chrome_test_profile\\StopWords_Currencies.txt\", \"r\")\n",
    "sw3 = open(\"E:\\sel\\chrome_test_profile\\StopWords_DatesandNumbers.txt\", \"r\")\n",
    "sw4 = open(\"E:\\sel\\chrome_test_profile\\StopWords_Generic.txt\", \"r\")\n",
    "sw5 = open(\"E:\\sel\\chrome_test_profile\\StopWords_GenericLong.txt\", \"r\")\n",
    "sw6 = open(\"E:\\sel\\chrome_test_profile\\StopWords_Geographic.txt\", \"r\")\n",
    "sw7 = open(\"E:\\sel\\chrome_test_profile\\StopWords_Names.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw1_text = sw1.read()\n",
    "sw2_text = sw2.read()\n",
    "sw3_text = sw3.read()\n",
    "sw4_text = sw4.read()\n",
    "sw5_text = sw5.read()\n",
    "sw6_text = sw6.read()\n",
    "sw7_text = sw7.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stopwords = \"\"\n",
    "new_stopwords += sw1_text\n",
    "new_stopwords += sw2_text\n",
    "new_stopwords += sw3_text\n",
    "new_stopwords += sw4_text\n",
    "new_stopwords += sw5_text\n",
    "new_stopwords += sw6_text\n",
    "new_stopwords += sw7_text\n",
    "#new_stopword= new_stopwords.split('|')\n",
    "#new_stopwords1=new_stopword.split('\\n')\n",
    "#print(new_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "str0 = new_stopwords.replace('\\n', ' ')\n",
    "str1 = str0.replace('|', ' ')\n",
    "list_1 = str1.split()\n",
    "#print(list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.append(list_1)\n",
    "#print(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw=open(r\"E:\\sel\\chrome_test_profile\\negative-words.txt\", \"r\")\n",
    "pw = open(r\"E:\\sel\\chrome_test_profile\\positive-words.txt\", \"r\")\n",
    "nw_text = nw.read()\n",
    "pw_text = pw.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = nw_text.replace('\\n', ' ')\n",
    "neg_list = negative.split()\n",
    "#print(neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pw_text.replace('\\n', ' ')\n",
    "pos_list = positive.split()\n",
    "#print(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "posScore=[]\n",
    "negScore=[]\n",
    "polar_score=[]\n",
    "subject_score=[]\n",
    "avg_sen_len=[]\n",
    "per_complex_words=[]\n",
    "fog_ind=[]\n",
    "avg_num_words=[]\n",
    "complex_word_count=[]\n",
    "word_count=[]\n",
    "syllable_per_word=[]\n",
    "personal_pronoun=[]\n",
    "avg_word_length=[]\n",
    "\n",
    "\n",
    "\n",
    "def pos_score(word):\n",
    "    global positive_score\n",
    "    positive_score=0\n",
    "    for x in word:\n",
    "        if x in pos_list:\n",
    "            positive_score=positive_score+1\n",
    "    #print(positive_score)\n",
    "    posScore.append(positive_score)\n",
    "\n",
    "\n",
    "def neg_score(word):\n",
    "    global negative_score\n",
    "    negative_score=0\n",
    "    for x in word:\n",
    "        if x in neg_list:\n",
    "            negative_score=negative_score+1\n",
    "    #print(negative_score)\n",
    "    negScore.append(negative_score)\n",
    "\n",
    "    \n",
    "def pol_score():\n",
    "    global polarity_score\n",
    "    polarity_score = (positive_score-negative_score)/(positive_score+negative_score)\n",
    "    #print(polarity_score)\n",
    "    polar_score.append(polarity_score)\n",
    "\n",
    "\n",
    "def sub_score():\n",
    "    global subjectivity_score\n",
    "    subjectivity_score = (positive_score+negative_score)/(total_words+0.000001)\n",
    "    #print(subjectivity_score)\n",
    "    subject_score.append(subjectivity_score)\n",
    "\n",
    "\n",
    "def avg_sent_len():\n",
    "    global avg_sentence_length\n",
    "    avg_sentence_length = (total_words/total_sent)\n",
    "    #print(avg_sentence_length)\n",
    "    avg_sen_len.append(avg_sentence_length)\n",
    "\n",
    "\n",
    "def no_of_words():\n",
    "    my_list=[]\n",
    "    for i in tokenized_sent:\n",
    "        words = i.split()\n",
    "        no_of_words = len(words)\n",
    "        my_list.append(no_of_words)\n",
    "        \n",
    "    avg_word_in_sent = (sum(my_list)/len(my_list))\n",
    "    #print(avg_word_in_sent)\n",
    "    avg_num_words.append(avg_word_in_sent)\n",
    "\n",
    "\n",
    "def avg_word_len():\n",
    "    for i in tokenized_sent:\n",
    "        words = i.split()\n",
    "    avg_len=(sum(map(len, words))/len(words))\n",
    "    #print(avg_len)\n",
    "    avg_word_length.append(avg_len)\n",
    "\n",
    "\n",
    "def no_pronouns():\n",
    "    pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
    "    pronouns = pronounRegex.findall(str(tokenized_sent))\n",
    "    #print(tokenized_sent)\n",
    "    no_pronouns = len(pronouns)\n",
    "    #print(no_pronouns)\n",
    "    personal_pronoun.append(no_pronouns)\n",
    "\n",
    "\n",
    "def syllable_count():\n",
    "    global syl_count\n",
    "    syl_count=[]\n",
    "    for word in filtered_word:\n",
    "        \n",
    "        word = word.lower()\n",
    "        #print(word)\n",
    "        count = 0\n",
    "        vowels = \"aeiouy\"\n",
    "        if word[0] in vowels:\n",
    "            count += 1\n",
    "        for index in range(1, len(word)):\n",
    "            if word[index] in vowels and word[index - 1] not in vowels:\n",
    "                count += 1\n",
    "        if word.endswith(\"es\" and \"ed\"):\n",
    "            count -= 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        #return count\n",
    "        syl_count.append(count)\n",
    "        #print(count)    \n",
    "        #print(syllable_count())\n",
    "    #print(syl_count)\n",
    "    avg_syl=(sum(syl_count)/len(syl_count))\n",
    "    #print(avg_syl)\n",
    "    syllable_per_word.append(avg_syl)\n",
    "\n",
    "def complex_word():\n",
    "    global comp_wrd_count\n",
    "    comp_wrd_count=0\n",
    "    for i in syl_count:\n",
    "        if(i>2):\n",
    "            comp_wrd_count += 1\n",
    "    #print(comp_wrd_count)\n",
    "    complex_word_count.append(comp_wrd_count)\n",
    "\n",
    "def per_comp_words():\n",
    "    global per_com_words\n",
    "    per_com_words=((comp_wrd_count)/(total_words))\n",
    "    #print(per_com_words)\n",
    "    per_complex_words.append(per_com_words)\n",
    "\n",
    "\n",
    "def fog_index():\n",
    "    fog_index=(0.4*(avg_sentence_length+per_com_words))\n",
    "    #print(fog_index)\n",
    "    fog_ind.append(fog_index)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(url_id1)):\n",
    "    url_id=url_id1[i]\n",
    "    file=open(f\"{url_id}.txt\",\"r\",encoding=\"utf-8\")\n",
    "    file_text=file.read()\n",
    "    global total_sent\n",
    "    global tokenized_sent\n",
    "    tokenized_sent=sent_tokenize(file_text)\n",
    "    total_sent = len(tokenized_sent)\n",
    "    #print(total_sent)\n",
    "    global total_words\n",
    "    global filtered_word\n",
    "    #tokenized_word=word_tokenize(file_text)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized_word=tokenizer.tokenize(file_text)\n",
    "    filtered_word=[]\n",
    "    for w in tokenized_word:\n",
    "        if w not in all_stopwords:\n",
    "            filtered_word.append(w)\n",
    "    #print(filtered_word)\n",
    "    total_words = len(filtered_word)\n",
    "    word_count.append(total_words)\n",
    "    #print(total_words)\n",
    "    \n",
    "    \n",
    "    pos_score(word = filtered_word)\n",
    "    neg_score(word = filtered_word)\n",
    "    pol_score()\n",
    "    sub_score()\n",
    "    avg_sent_len()\n",
    "    per_comp_words()\n",
    "    fog_index()\n",
    "    no_of_words()\n",
    "    complex_word()\n",
    "    syllable_count()\n",
    "    no_pronouns()\n",
    "    avg_word_len()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#print(posScore)\n",
    "#print(negScore)\n",
    "#print(polar_score)\n",
    "#print(subject_score)\n",
    "#print(avg_sen_len)\n",
    "#print(per_complex_words)\n",
    "#print(fog_ind)\n",
    "#print(avg_num_words)\n",
    "#print(complex_word_count)\n",
    "#print(word_count)\n",
    "#print(syllable_per_word)\n",
    "#print(personal_pronoun)\n",
    "#print(avg_word_length)\n",
    "\n",
    "\n",
    "header = [\"URL_ID\",\"URL\",\"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\",\"SUBJECTIVITY SCORE\", \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\", \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\",\"WORD COUNT\", \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"]\n",
    "output_data = zip(url_id1, url1,posScore,negScore,polar_score,subject_score,avg_sen_len, per_complex_words,fog_ind,avg_num_words,complex_word_count,word_count,syllable_per_word,personal_pronoun,avg_word_length )\n",
    "Output_Data_Structure = pd.DataFrame(list(output_data), columns = header)\n",
    "writer = pd.ExcelWriter(\"Output_Data_Structure.xlsx\", engine='xlsxwriter')\n",
    "Output_Data_Structure.to_excel(writer,sheet_name=\"Output Data Structure\", index=False)\n",
    "writer.save()\n",
    "\n",
    "\n",
    "#,posScore,negScore,polar_score,subject_score,avg_sen_len, per_complex_words,fog_ind,avg_num_words,complex_word_count,word_count,syllable_per_word,personal_pronoun,avg_word_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
